{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3bfcd96ed70346ce9fe026862a8c7409":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54001ac1a3b645b081c1da01272925b6","IPY_MODEL_9e9c267ae4c845d896407f0823e8906e","IPY_MODEL_91932ea40a4748abb6bdaf85638581a7"],"layout":"IPY_MODEL_c74b86d8d6e840c8b1fd15a04f448413"}},"54001ac1a3b645b081c1da01272925b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6a3827a1327410981111aba2be87b0b","placeholder":"​","style":"IPY_MODEL_5ab9a147f817473c93d34c094b76f387","value":"tf_model.h5: 100%"}},"9e9c267ae4c845d896407f0823e8906e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d796da409c046128b28bc6fe721e869","max":1459579344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4197edd072f14657b7ee111e4b8e8da8","value":1459579344}},"91932ea40a4748abb6bdaf85638581a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a38afaa19be44928ddf28a4a87e939e","placeholder":"​","style":"IPY_MODEL_53e534d5749748779c09005ee097ac06","value":" 1.46G/1.46G [00:25&lt;00:00, 46.2MB/s]"}},"c74b86d8d6e840c8b1fd15a04f448413":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6a3827a1327410981111aba2be87b0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ab9a147f817473c93d34c094b76f387":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d796da409c046128b28bc6fe721e869":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4197edd072f14657b7ee111e4b8e8da8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a38afaa19be44928ddf28a4a87e939e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53e534d5749748779c09005ee097ac06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vyom71/chatbot-using-huggingface?scriptVersionId=180736402\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# This is only for newbies in the journey of LLMs or Transformers ","metadata":{}},{"cell_type":"markdown","source":"# Here We will implement small task : Build Chatbot using simple methods , we will use opensource model from HuggingFace","metadata":{}},{"cell_type":"markdown","source":"Model link : https://huggingface.co/facebook/blenderbot-400M-distill ","metadata":{}},{"cell_type":"markdown","source":"So for Conversation Task we need model that is trained on conversation dataset so if you use other model that is not trained on specific model you will face problem called Irrelevant response, Chatbot may give answer same as your question is one of the problem.","metadata":{}},{"cell_type":"markdown","source":"So we have used blenderbot model that is trained on conversation. If we want specific model for any task we need to Fine Tune that model according to task I'll upload it later. Here let's see most basic AI chatbot you can try ! ","metadata":{}},{"cell_type":"markdown","source":"I have mentioned two methods here \n1. Direct Model and Tokenizer Use \n2. Using pipeline Utility","metadata":{}},{"cell_type":"markdown","source":"# 1. Direct Model and Tokenizer User","metadata":{}},{"cell_type":"markdown","source":"Here we imported tokenizer for blenderbot model it is must to use same tokenizer as model TFBlenderbotForConditionalGeneration is a class in the Hugging Face Transformers library designed to work with the Blenderbot model specifically within the TensorFlow framework.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import BlenderbotTokenizer, TFBlenderbotForConditionalGeneration","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:57:32.906877Z","iopub.execute_input":"2024-05-31T09:57:32.908146Z","iopub.status.idle":"2024-05-31T09:57:32.91227Z","shell.execute_reply.started":"2024-05-31T09:57:32.90811Z","shell.execute_reply":"2024-05-31T09:57:32.911217Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# to get rid of unnecessary errors\ntf.get_logger().setLevel('ERROR') ","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:58:08.614424Z","iopub.execute_input":"2024-05-31T09:58:08.615127Z","iopub.status.idle":"2024-05-31T09:58:08.621098Z","shell.execute_reply.started":"2024-05-31T09:58:08.615095Z","shell.execute_reply":"2024-05-31T09:58:08.620127Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Load tokenizer for Pre-trained model from blenderbot , now set padding token same as EOS token so padding that is added to input is same as output","metadata":{}},{"cell_type":"code","source":"tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:01:00.361887Z","iopub.execute_input":"2024-05-31T10:01:00.362265Z","iopub.status.idle":"2024-05-31T10:01:00.61381Z","shell.execute_reply.started":"2024-05-31T10:01:00.362235Z","shell.execute_reply":"2024-05-31T10:01:00.612808Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = TFBlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")\n# import model","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:01:34.007816Z","iopub.execute_input":"2024-05-31T10:01:34.008471Z","iopub.status.idle":"2024-05-31T10:01:37.98115Z","shell.execute_reply.started":"2024-05-31T10:01:34.008439Z","shell.execute_reply":"2024-05-31T10:01:37.980352Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"All model checkpoint layers were used when initializing TFBlenderbotForConditionalGeneration.\n\nSome layers of TFBlenderbotForConditionalGeneration were not initialized from the model checkpoint at facebook/blenderbot-400M-distill and are newly initialized: ['final_logits_bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Main function","metadata":{}},{"cell_type":"markdown","source":"firstly we apply tokenizer to input taken from user and apply padding and trucation if needed.\n","metadata":{}},{"cell_type":"markdown","source":"input_ids and attention_mask used to provide the model with the input text and info about which part is for input and which are for padding.","metadata":{}},{"cell_type":"markdown","source":"if you dont know why to use truncation follow this blog ( https://huggingface.co/docs/transformers/en/pad_truncation)","metadata":{}},{"cell_type":"markdown","source":"in output we have specified parameters and it's values : \n1. input_ids and attention_mask is done\n2. max_lenght -> maximum lenght of output/response ( 100 len )\n3. num_beams -> called beam search good for coherent searching \n4. no_repeat_ngram_size -> prevent from repeating phrases of 2 or more words\n5. early_stopping -> models stops if it is unlikely to produce better results\n6. temprature -> controll randomness of response if 0 mean no randomness.","metadata":{}},{"cell_type":"markdown","source":"all are hyperparameter should be tunned for better results","metadata":{}},{"cell_type":"code","source":"def chat():\n    while True:\n        text = input(\"You: \")\n        if text.lower() == 'quit':\n            break\n\n        inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)\n        input_ids = inputs['input_ids']\n        attention_mask = inputs['attention_mask']\n\n        output = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=100,\n            num_beams=5,\n            no_repeat_ngram_size=2,\n            early_stopping=True,\n            temperature=0.8\n        )\n\n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        print(\"Chatbot: \" + response)\n\nchat()","metadata":{"execution":{"iopub.status.busy":"2024-05-31T10:12:21.745237Z","iopub.execute_input":"2024-05-31T10:12:21.746159Z","iopub.status.idle":"2024-05-31T10:13:50.705469Z","shell.execute_reply.started":"2024-05-31T10:12:21.746125Z","shell.execute_reply":"2024-05-31T10:13:50.704703Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdin","text":"You:  Hi, How are you ?\n"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Chatbot:  I'm doing well. How are you doing? I just got back from a walk with my dog.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  My name is Vyom , your ?\n"},{"name":"stdout","text":"Chatbot:  Hi Vyom, my name is Meg. Do you have any pets?  <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Yes I have one Labradore\n"},{"name":"stdout","text":"Chatbot:  Labradors are great dogs. I have one myself. Do you have any other pets?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  No \n"},{"name":"stdout","text":"Chatbot:  Do you have any advice on how to get out of this situation? I am so scared. <pad> <pad> <pad> <pad> <pad>\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  Quit\n"}]},{"cell_type":"markdown","source":"Now in this method we have used direct pipeline from transformers here code is less but you will get better result in 1st method as we can do hyperparameter tunning.","metadata":{}},{"cell_type":"markdown","source":"# 2. Using Pipeline Utility ","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, Conversation\ntf.get_logger().setLevel('ERROR')\n\nchatbot = pipeline('conversational', model='facebook/blenderbot-400M-distill')\n\n\ndef chat():\n    conversation = Conversation()\n    while True:\n        text = input(\"You: \")\n        if text.lower() == 'quit':\n            break\n\n        conversation.add_user_input(text)\n\n        response = chatbot(conversation)\n\n\n        print(\"Chatbot: \" + response.generated_responses[-1])\n\nchat()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-31T09:40:19.710943Z","iopub.execute_input":"2024-05-31T09:40:19.711704Z","iopub.status.idle":"2024-05-31T09:41:44.127005Z","shell.execute_reply.started":"2024-05-31T09:40:19.711672Z","shell.execute_reply":"2024-05-31T09:41:44.125905Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/730M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54add7b2c04a440a8bca3a9d61e2d2e9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  hi\n"},{"name":"stderr","text":"No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n","output_type":"stream"},{"name":"stdout","text":"Chatbot:  Hi! How are you? I just got back from walking my dog. Do you have any pets?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  I am fine \n"},{"name":"stdout","text":"Chatbot:  That's good. What kind of dog do you have? I have a poodle.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  I have Labradore \n"},{"name":"stdout","text":"Chatbot:  I love labradors! I used to have one, but she passed away a few years ago.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  quit\n"}]},{"cell_type":"markdown","source":"# Which is better ?","metadata":{}},{"cell_type":"markdown","source":"1. Pipelines:  Designed for simplicity. They offer a streamlined way to perform common NLP tasks (like text generation, sentiment analysis, etc.) with pre-defined configurations. While convenient, they might not be suitable if you need more control over the model's behavior.\n\n2. Direct Model Usage:  Gives you full control over the model's input and output. You can fine-tune the model's parameters, choose specific decoding strategies (beam search, top-k/top-p sampling), and modify pre- and post-processing steps as needed.","metadata":{}}]}